# VGG6 CIFAR-10 Deep Learning Assignment

A comprehensive PyTorch implementation of VGG6 neural network for CIFAR-10 image classification with hyperparameter optimization, optimizer comparison, and activation function analysis.

## üìã Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Installation & Setup](#installation--setup)
- [How to Run](#how-to-run)
- [Assignment Questions](#assignment-questions)
- [Hyperparameter Analysis](#hyperparameter-analysis)
- [Results & Performance](#results--performance)
- [Architecture Details](#architecture-details)
- [Advanced Features](#advanced-features)
- [Troubleshooting](#troubleshooting)
- [Dependencies](#dependencies)

## üéØ Overview

This repository contains three complete assignments implementing VGG6 architecture for CIFAR-10 classification:

- **Question 1**: Basic VGG6 implementation with standard training
- **Question 2**: Optimizer comparison across 8 different algorithms
- **Question 3**: **Advanced hyperparameter optimization** with W&B integration and parallel coordinate analysis

**Best Results Achieved**: **76.37% test accuracy** on CIFAR-10 using optimized hyperparameters.

## ‚ú® Features

### üèóÔ∏è **Optimized VGG6 Architecture**
- 6 convolutional layers with batch normalization
- 3 max pooling layers for spatial downsampling
- 2 fully connected layers with dropout regularization
- **693,322 parameters** - compact yet effective
- Designed specifically for 32√ó32√ó3 CIFAR-10 images

### üöÄ **Advanced Data Augmentation**
- **AutoAugment CIFAR10 policy** - state-of-the-art augmentation
- **Cutout regularization** - randomly mask image patches
- Random cropping with padding and horizontal flipping
- Per-channel normalization with CIFAR-10 statistics

### üíª **Cross-Platform Support**
- **NVIDIA CUDA** GPU acceleration
- **Apple Silicon** (M1/M2/M3) MPS optimization
- CPU fallback support
- Automatic hardware detection and optimization

### üî¨ **Hyperparameter Optimization**
- **Weights & Biases** integration for experiment tracking
- **Bayesian optimization** across 6 hyperparameters
- **Parallel coordinate plots** for visualization
- **Real-time training metrics** logging

### üìä **Comprehensive Analysis**
- **8 optimizer comparison**: SGD variants, Adam family, adaptive methods
- **5 activation functions**: ReLU, GELU, SiLU, Sigmoid, Tanh
- **Training visualization**: Loss curves, accuracy plots, validation analysis
- **Best configuration identification** with performance metrics

## üõ†Ô∏è Installation & Setup

### 1. Clone Repository
```bash
git clone <repository-url>
cd assignment-1
```

### 2. Create Virtual Environment (Recommended)
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies
```bash
# Core dependencies
pip install torch torchvision numpy pillow

# For W&B integration (Question 3)
pip install wandb pandas matplotlib seaborn

# For visualization
pip install plotly  # Optional for enhanced plots
```

### 4. Login to W&B (For Question 3)
```bash
wandb login
```
Follow the prompts to authenticate with your W&B account.

## üöÄ How to Run

### Quick Start - Best Results
For the **complete hyperparameter optimization experience** (recommended):
```bash
python "vgg6-question-3-assignment .py"
```
This will automatically run the **Bayesian hyperparameter sweep** and generate **parallel coordinate plots**.

### Individual Assignments

#### **Question 1: Basic VGG6 Implementation**
```bash
python vgg6-question-1-assignment.py
```
- Trains VGG6 with default hyperparameters
- Generates basic training/validation curves
- Saves model checkpoints

#### **Question 2: Optimizer Comparison**
```bash
python vgg6-question-2-assignment.py
```
Tests all 8 optimizers:
- SGD (vanilla, momentum, Nesterov)
- Adam, AdamW, Adagrad, RMSprop, NAdam

#### **Question 3: Advanced Hyperparameter Analysis**
```bash
python "vgg6-question-3-assignment .py"
```
**Features:**
- **20 hyperparameter combinations** via Bayesian optimization
- **Real-time W&B logging** with interactive dashboards
- **Parallel coordinate plots** showing hyperparameter relationships
- **Best configuration identification**
- **Automatic result visualization**

### üéõÔ∏è Configuration Options

Edit the `CONFIG` dictionary in any script to customize:

```python
CONFIG = {
    # W&B Configuration (Question 3)
    'RUN_WANDB_SWEEP': True,          # Enable hyperparameter sweep
    'PREVIEW_WANDB_SWEEP': False,     # Show config without running
    
    # Alternative Testing Modes
    'TEST_ALL_OPTIMIZERS': False,     # Sequential optimizer testing
    'TEST_ALL_ACTIVATIONS': False,    # Activation function analysis
    'TEST_HYPERPARAMETERS': False,    # Predefined config testing
    
    # Training Parameters
    'EPOCHS': 10,                     # Training epochs
    'BATCH_SIZE': 128,               # Mini-batch size
    'LEARNING_RATE': 0.001,          # Base learning rate
    'WEIGHT_DECAY': 0.001,           # L2 regularization
}
```

## üìù Assignment Questions

### **Question 1**: Basic Implementation
- ‚úÖ Implement VGG6 architecture for CIFAR-10
- ‚úÖ Train with standard hyperparameters
- ‚úÖ Achieve baseline performance
- ‚úÖ Generate training curves

### **Question 2**: Optimizer Analysis
- ‚úÖ Compare 8 different optimizers
- ‚úÖ Analyze convergence characteristics
- ‚úÖ Document performance differences
- ‚úÖ Provide optimization recommendations

### **Question 3**: Hyperparameter Optimization
- ‚úÖ Implement W&B hyperparameter sweep
- ‚úÖ **Create parallel coordinate plots**
- ‚úÖ **Answer: "Which configuration achieves what accuracy?"**
- ‚úÖ **Best result: 76.37% test accuracy**
- ‚úÖ Generate comprehensive analysis

## üîç Hyperparameter Analysis

### Hyperparameters Explored
| Parameter | Values | Impact |
|-----------|--------|--------|
| **Batch Size** | [32, 64, 128, 256] | Memory vs. convergence stability |
| **Learning Rate** | [0.0001, 0.0005, 0.001, 0.005, 0.01] | Most critical parameter |
| **Optimizer** | ['adam', 'sgd', 'rmsprop', 'adamw'] | Algorithm choice |
| **Activation** | ['relu', 'gelu', 'silu'] | Nonlinearity selection |
| **Epochs** | [5, 10, 15] | Training duration |
| **Weight Decay** | [0.0001, 0.001, 0.01] | Regularization strength |

### üèÜ Best Configuration Found
```python
# Achieved 76.37% Test Accuracy
{
    'batch_size': 32,
    'learning_rate': 0.005,
    'optimizer': 'sgd',
    'activation': 'gelu', 
    'epochs': 15,
    'weight_decay': 0.001
}
```

### Key Insights
1. **SGD outperformed adaptive optimizers** (Adam, RMSprop) for this architecture
2. **GELU activation** provided best results vs ReLU/SiLU
3. **Learning rate 0.005** hit optimal balance
4. **Smaller batch size (32)** improved convergence
5. **15 epochs** sufficient for peak performance

## üìà Results & Performance

### Performance Summary
- **Best Test Accuracy**: **76.37%** (excellent for CIFAR-10)
- **Mean Accuracy**: 58.61% across all 20 configurations
- **Standard Deviation**: 17.24% (good hyperparameter sensitivity)
- **Model Parameters**: 693,322 (compact architecture)

### Training Examples
**Best Run (SGD + GELU):**
- Epoch 1: 23% ‚Üí Epoch 15: 65% training accuracy
- Final test accuracy: 77.48% 
- Excellent convergence with minimal overfitting

**Poor Run (RMSprop + high LR):**
- Plateaued at ~20% accuracy
- Demonstrates importance of hyperparameter tuning

### Visualization
- **W&B Dashboard**: Interactive parallel coordinate plots
- **Training Curves**: Real-time loss and accuracy monitoring  
- **Hyperparameter Importance**: Statistical analysis of parameter impact
- **Configuration Comparison**: Side-by-side performance analysis

## üèóÔ∏è Architecture Details

### VGG6 Network Structure
```
Input: (3, 32, 32) CIFAR-10 images

Block 1: Conv(64) ‚Üí Conv(64) ‚Üí MaxPool  ‚Üí (64, 16, 16)
Block 2: Conv(128) ‚Üí Conv(128) ‚Üí MaxPool ‚Üí (128, 8, 8) 
Block 3: Conv(256) ‚Üí MaxPool             ‚Üí (256, 4, 4)

Global Average Pool                      ‚Üí (256, 1, 1)
Flatten                                  ‚Üí (256,)

Classifier: Linear(256‚Üí512) ‚Üí Activation ‚Üí Dropout(0.5) ‚Üí Linear(512‚Üí10)
Output: (10,) class logits
```

### Advanced Features
- **Batch Normalization**: After each convolutional layer
- **Dropout Regularization**: 50% in classifier to prevent overfitting
- **Adaptive Global Pooling**: Handles variable input sizes
- **Smart Weight Initialization**: Kaiming/Xavier based on activation
- **Learning Rate Scheduling**: ReduceLROnPlateau for adaptive adjustment

## üîß Advanced Features

### Data Pipeline
```python
# Training Augmentation
transforms.Compose([
    transforms.RandomCrop(32, padding=4),    # Random cropping
    transforms.RandomHorizontalFlip(),       # 50% horizontal flip
    AutoAugmentCIFAR10(),                    # Advanced augmentation
    Cutout(n_holes=1, length=16),           # Cutout regularization
    transforms.ToTensor(),
    transforms.Normalize(mean, std)           # CIFAR-10 normalization
])
```

### Optimizer Configuration
```python
# Support for 8 optimizers with optimal settings
optimizers = {
    'sgd': optim.SGD(lr=lr, momentum=0.9, weight_decay=wd),
    'adam': optim.Adam(lr=lr, betas=(0.9, 0.999), weight_decay=wd),
    'adamw': optim.AdamW(lr=lr, betas=(0.9, 0.999), weight_decay=wd),
    'rmsprop': optim.RMSprop(lr=lr, alpha=0.99, weight_decay=wd),
    # ... and more
}
```

### W&B Integration
```python
# Automatic experiment tracking
wandb.log({
    'epoch': epoch,
    'train_accuracy': train_acc,
    'test_accuracy': test_acc,
    'train_loss': train_loss,
    'learning_rate': current_lr,
    'best_test_accuracy': best_acc
})
```

## üîß Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```python
# Reduce batch size in CONFIG
CONFIG['BATCH_SIZE'] = 64  # or 32
```

**2. W&B Authentication**
```bash
wandb login --relogin
```

**3. MPS (Apple Silicon) Issues**
```python
# Set pin_memory=False for MPS
DataLoader(..., pin_memory=False)
```

**4. Module Import Errors**
```bash
# Install missing dependencies
pip install wandb pandas matplotlib seaborn
```

**5. Slow Training**
```python
# Enable hardware acceleration
device = torch.device("mps" if torch.backends.mps.is_available() else 
                     "cuda" if torch.cuda.is_available() else "cpu")
```

### Performance Optimization
- **GPU Memory**: Use mixed precision training for larger models
- **Data Loading**: Set `num_workers=4` in DataLoader for faster I/O
- **Batch Size**: Larger batches for better GPU utilization
- **Learning Rate**: Scale with batch size: `lr = base_lr * batch_size / 256`

## üì¶ Dependencies

### Core Requirements
```txt
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.21.0
pillow>=8.0.0
```

### W&B Integration
```txt
wandb>=0.15.0
pandas>=1.3.0
matplotlib>=3.5.0
seaborn>=0.11.0
```

### Optional Enhancements
```txt
plotly>=5.0.0        # Interactive plots
tensorboard>=2.8.0   # Alternative logging
tqdm>=4.62.0         # Progress bars
```

### Installation Command
```bash
pip install torch torchvision numpy pillow wandb pandas matplotlib seaborn
```

## üéØ Quick Results

Want to see the best results immediately? Run:

```bash
# 1. Full hyperparameter sweep (recommended)
python "vgg6-question-3-assignment .py"

# 2. Check W&B dashboard at:
# https://wandb.ai/your-username/vgg6-hyperparameter-sweep

# 3. View parallel coordinate plots showing:
# "Which configuration achieves what accuracy?"
```

**Expected Results:**
- üéØ **76.37% best test accuracy**
- üìä **20 hyperparameter combinations tested**
- ‚è±Ô∏è **~6-7 hours total runtime** (20 min per run)
- üìà **Interactive parallel coordinate plots**
- üîç **Optimal configuration identification**

---

## üìû Support

For issues or questions:
1. Check the [Troubleshooting](#troubleshooting) section
2. Review W&B dashboard for experiment details
3. Verify all dependencies are installed correctly
4. Ensure proper hardware acceleration is enabled

**Happy experimenting! üöÄ**

## Usage

### Environment Setup

1. Create and activate a virtual environment:

   ```bash
   # Using venv (Python's built-in virtual environment)
   python -m venv venv
   
   # On macOS/Linux
   source venv/bin/activate
   
   # On Windows
   .\venv\Scripts\activate
   ```

2. Install PyTorch based on your system:

   ```bash
   # For CUDA (NVIDIA GPU)
   pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118

   # For Apple Silicon (M1/M2)
   pip3 install torch torchvision
   
   # For CPU only
   pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu
   ```

3. Install other dependencies:
   ```bash
   pip install numpy pillow
   ```

4. Verify the installation:
   ```python
   python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
   python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
   python -c "import torch; print(f'MPS available: {torch.backends.mps.is_available()}')"
   ```

### Running the Training

Run the training script:
```bash
python vgg6-question-1-assignment.py
```

The script will automatically:
1. Detect available hardware (CUDA/MPS/CPU)
2. Download and prepare CIFAR-10 dataset
3. Initialize the VGG6 model
4. Train for the specified number of epochs
5. Save the best performing model

### Common Issues and Solutions

1. CUDA Out of Memory:
   - Reduce `BATCH_SIZE` in the CONFIG dictionary
   - Close other GPU-intensive applications

2. Slow Training on CPU:
   - Ensure you have the correct PyTorch version for your hardware
   - Reduce the number of worker processes if system is overloaded

3. Apple Silicon (M1/M2) Issues:
   - Ensure you have Rosetta 2 installed if using x86 Python
   - Use native ARM64 Python for best performance
   - Update to latest PyTorch version for MPS optimization

## Configuration

Key hyperparameters can be modified in the `CONFIG` dictionary:

```python
CONFIG = {
    'SEED': 42,              # Random seed
    'NUM_CLASSES': 10,       # CIFAR-10 classes
    'BATCH_SIZE': 128,       # Training batch size
    'LEARNING_RATE': 0.01,   # Initial learning rate
    'WEIGHT_DECAY': 0.001,   # L2 regularization
    'EPOCHS': 100,           # Training epochs
}
```

## Architecture

The VGG6 architecture consists of:

```
Input (32x32x3)
‚îÇ
‚îú‚îÄ‚îÄ Conv2D(64) ‚Üí BN ‚Üí ReLU
‚îú‚îÄ‚îÄ Conv2D(64) ‚Üí BN ‚Üí ReLU
‚îú‚îÄ‚îÄ MaxPool2D
‚îÇ
‚îú‚îÄ‚îÄ Conv2D(128) ‚Üí BN ‚Üí ReLU
‚îú‚îÄ‚îÄ Conv2D(128) ‚Üí BN ‚Üí ReLU
‚îú‚îÄ‚îÄ MaxPool2D
‚îÇ
‚îú‚îÄ‚îÄ Conv2D(256) ‚Üí BN ‚Üí ReLU
‚îú‚îÄ‚îÄ MaxPool2D
‚îÇ
‚îú‚îÄ‚îÄ AdaptiveAvgPool2D
‚îÇ
‚îú‚îÄ‚îÄ Linear(256‚Üí512) ‚Üí ReLU ‚Üí Dropout(0.5)
‚îî‚îÄ‚îÄ Linear(512‚Üí10)
```

## Performance Optimizations

- **Data Loading**
  - Adaptive number of worker processes
  - Persistent workers
  - Prefetch queue
  - Pinned memory when available

- **Training**
  - Batch normalization for faster convergence
  - Learning rate scheduling
  - Memory-efficient data augmentation
  - Platform-specific optimizations

## License

[Specify your license here]

## Author

Joshua Sumanth Kumar Relton